{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building MAML From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section we saw how MAML works. We saw how MAML obtains a better and robust model parameter $\\theta$ that is generalizable across tasks. \n",
    "\n",
    "\n",
    "Now we will better understand MAML by coding them from scratch. For better understanding, we consider a simple binary classification task. We randomly generate our input data and we train them with a simple single layer neural network and try to find the optimal parameter theta. \n",
    "\n",
    "Now we will step by step how exacly we are doing this,\n",
    "\n",
    "First we import all the necessary libraries,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AToy.csv')\n",
    "df = df.drop(df.columns[0], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function called sample_points for generating our input (x,y) pairs. It takes the parameter k as an input which implies number of (x,y) pairs we want to sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(k, l):\n",
    "    x1 = df.loc[k:l-1,['A.Open', 'A.High', 'A.Low', 'A.Close', 'A.Volume', 'A.Adjusted']]\n",
    "    y1 = df.loc[k:l-1,['Y']]\n",
    "    x2 = np.array(x1.values.tolist())\n",
    "    y2 = np.array(y1.values.tolist())\n",
    "    scaler0 = MinMaxScaler()\n",
    "    scaler1 = MinMaxScaler()\n",
    "    scaler0.fit(x2)\n",
    "    scaler1.fit(y2)\n",
    "    x = scaler0.transform(x2)\n",
    "    y = scaler1.transform(y2)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function returns output as follows, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sample_points(10)\n",
    "# print(x[0])\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df[['A.Open', 'A.High', 'A.Low', 'A.Close', 'A.Volume', 'A.Adjusted']]\n",
    "# Y_train = df[['Y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and better understand, we use a neural network with only single layer for predicting the output. i.e,\n",
    "\n",
    "a = np.matmul(X, theta)\n",
    "\n",
    "YHat = sigmoid(a)\n",
    "\n",
    "\n",
    "\n",
    "__*So, we use MAML for finding this optimal parameter value theta that is generalizable across tasks. So that \n",
    "for a new task, we can learn from a few data points in a lesser time by taking very less gradient steps.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a class called MAML where we implement the MAML algorithm. In the \\__init__  method we will initialize all the necessary variables. Then we define our sigmoid activation function. Followed by we define our train function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the comments written above each line of code for understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #initialize number of tasks i.e number of tasks we need in each batch of tasks\n",
    "        self.num_tasks = 1\n",
    "        \n",
    "        #number of samples i.e number of shots  -number of data points (k) we need to have in each task\n",
    "        self.num_samples = 20\n",
    "\n",
    "        #number of epochs i.e training iterations\n",
    "        self.epochs = 10000\n",
    "        \n",
    "        #hyperparameter for the inner loop (inner gradient update)\n",
    "        self.alpha = 0.0001\n",
    "        \n",
    "        #hyperparameter for the outer loop (outer gradient update) i.e meta optimization\n",
    "        self.beta = 0.0001\n",
    "       \n",
    "        #randomly initialize our model parameter theta\n",
    "        self.theta = np.random.normal(size=6).reshape(6, 1)\n",
    "      \n",
    "    #define our sigmoid activation function  \n",
    "    def sigmoid(self,a):\n",
    "        return 1.0 / (1 + np.exp(-a))\n",
    "    \n",
    "    \n",
    "    #now let us get to the interesting part i.e training :P\n",
    "    def train(self):\n",
    "        \n",
    "        #for the number of epochs,\n",
    "        for e in range(self.epochs):        \n",
    "            \n",
    "            self.theta_ = []\n",
    "            \n",
    "            #for task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "               \n",
    "                #sample k data points and prepare our train set\n",
    "                XTrain, YTrain = sample_points(0,20)\n",
    "#                 XTrain = X_train[:20]\n",
    "#                 YTrain = Y_train[:20]\n",
    "                \n",
    "                a = np.matmul(XTrain, self.theta)\n",
    "#                 print(a)\n",
    "\n",
    "                YHat = self.sigmoid(a)\n",
    "#                 print(YHat)\n",
    "\n",
    "                #since we are performing classification, we use cross entropy loss as our loss function\n",
    "                loss = ((np.matmul(-YTrain.T, np.log(YHat)) - np.matmul((1 -YTrain.T), np.log(1 - YHat)))/self.num_samples)[0][0]\n",
    "#                 print(loss)\n",
    "                #minimize the loss by calculating gradients\n",
    "                gradient = np.matmul(XTrain.T, (YHat - YTrain)) / self.num_samples\n",
    "\n",
    "                #update the gradients and find the optimal parameter theta' for each of tasks\n",
    "                self.theta_.append(self.theta - self.alpha*gradient)\n",
    "                \n",
    "     \n",
    "            #initialize meta gradients\n",
    "            meta_gradient = np.zeros(self.theta.shape)\n",
    "                        \n",
    "            for i in range(self.num_tasks):\n",
    "            \n",
    "                #sample k data points and prepare our test set for meta training\n",
    "                XTest, YTest = sample_points(20,24)\n",
    "#                 XTest = X_train[20:23]\n",
    "#                 YTest = Y_train[20:23]\n",
    "                \n",
    "\n",
    "                #predict the value of y\n",
    "                a = np.matmul(XTest, self.theta_[i])\n",
    "                \n",
    "                YPred = self.sigmoid(a)\n",
    "                           \n",
    "                #compute meta gradients\n",
    "                meta_gradient += np.matmul(XTest.T, (YPred - YTest)) / self.num_samples\n",
    "\n",
    "  \n",
    "            #update our randomly initialized model parameter theta with the meta gradients\n",
    "            self.theta = self.theta-self.beta*meta_gradient/self.num_tasks\n",
    "#             print(\"THeta: {}\\n\".format(self.theta))  \n",
    "#             print(self.theta_)\n",
    "#             if e%1000==0:\n",
    "#                 print(\"Epoch {}: Loss {}\\n\".format(e,loss))             \n",
    "#                 print ('Updated Model Parameter Theta\\n')\n",
    "#                 print ('Sampling Next Batch of Tasks \\n')\n",
    "#                 print ('---------------------------------\\n')\n",
    "    def test(self):\n",
    "        XTest_, YTest_ = sample_points(24, 27)\n",
    "#         XTest_ = X_train[23:]\n",
    "#         YTest_ = Y_train[23:]\n",
    "        a_ = np.matmul(XTest_, self.theta)\n",
    "        YPred_ = self.sigmoid(a_)\n",
    "#         print(\"Test {}\\n\".format(YPred_))\n",
    "#         print(\"Actual {}\\n\".format(YTest_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
